# -*- coding: utf-8 -*-
"""practica2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18nuwFizRRkTJ3XyZFgzerwNmfj58Mdou

<center> <b> PRÁCTICA 2 - TIPOLOGÍA Y CICLO DE VIDA DE LOS DATOS </b> </center>

Alicia Amores Sánchez


Carlos Núñez Arilla

**1. Descripción del dataset.** ¿Por qué es importante y qué pregunta/problema
pretende responder?

El dataset contiene información de diferentes puestos de trabajo relacionados con la ciencia de datos y los salarios mínimos, medios y máximos de cada uno de ellos. 
En función de estas características se quiere ver qué puesto obtiene mejores salarios y se quiere predecir los salarios máximos para cada tipo de puesto en función de las características descritas. 

Algunas características del puesto de trabajo pueden ser: 
la compañía, el tamaño de la compañía, la localización del puesto, los conocimientos del candidato...

**2. Integración y selección de los datos de interés a analizar.** Puede ser el
resultado de adicionar diferentes datasets o una subselección útil de los datos
originales, en base al objetivo que se quiera conseguir.
"""

#Importación de paquetes a utilizar

import pandas as pd
import numpy as np
import missingno
import matplotlib.pyplot as plt
import missingno

#carga de datos
data=pd.read_csv('eda_data.csv')
data.head()

"""La columna Unnamed no aporta información relevante, con lo que la eliminamos para el análisis. """

data.drop('Unnamed: 0', inplace=True, axis=1)
data

"""**3. Limpieza de los datos.**


**3.1.** ¿Los datos contienen ceros o elementos vacíos? Gestiona cada uno de
estos casos.


**3.2.** Identifica y gestiona los valores extremos.

**Datos Faltantes**
"""

# Lista con los missing de cada columna
missing = data.isna().sum()
# Creamos dataframe con columnas y missing values
missing_df = pd.DataFrame([data.columns,missing]).transpose()
# Nos quedamos solo con  las columnas que tengan missing
for index in missing_df.index:
    if missing_df.iloc[index,1] > 0:
        print('Columna: {}, N of NaN: {}, Index in Dataframe: {}'.format(missing_df.iloc[index,0],missing_df.iloc[index,1],index))

"""No existen datos faltantes. En la siguiente figura se puede observar de forma visual. En caso de haber espacios en blanco, estos indicarían donde están los datos faltantes."""

missingno.matrix(data,sparkline=False, figsize=(10,5), fontsize=12, color=(0.27, 0.52, 1.0))

"""**Análisis de variables categóricas**"""

data.describe(include='object').T

"""En la columna *seniority* la mayoría de los valores son na. No se han reconocido como valores nulos, sino como string, pero relamente son valores vacíos que no aportan ninguna información. Por ello se ha decidido eliminar dicha columna. 

Por otro lado, la mayoría de los valores del campo *Competitors* son -1, lo cual no tiene mucho sentido, por ello también se va a eliminar dicha columna. 

La columna de *Salary Estimate* lo podemos determinar en función del salario medio. 

Por último también vamos a eliminar la columna de *job description* ya que nos vamos a enfocar solamente en los roles de Data Scientist y Data Engineer en el modelo estadístico. 
"""

data.drop(['seniority', 'Competitors', 'Salary Estimate', 'Job Description'], inplace=True, axis=1)
data.head()

"""**Análisis de variables numéricas**"""

stats_df = data.describe()
# Obtener y añadir la mediana
median = pd.DataFrame(data.median())
median = median.transpose()
median.rename(index = {0:'median'},inplace = True)
stats_df = stats_df.append(median).transpose() # Hay q igualarlo porq append() devuelve nuevo df
stats_df

"""Hay valores cuyo mínimo es -1 y no aplicaría, por lo que vamos a sustituir dichos valores por la media."""

for i in range(0, len(data)):
  if data['Rating'][i]==-1:
    data['Rating'][i]=data['Rating'].median()
  if data['Founded'][i]==-1:
    data['Founded'][i]=data['Founded'].median()
  if data['age'][i]<18 or data['age'][i]>60:
    data['age'][i]=data['age'].median()

stats_df = data.describe()
# Obtener y añadir la mediana
median = pd.DataFrame(data.median())
median = median.transpose()
median.rename(index = {0:'median'},inplace = True)
stats_df = stats_df.append(median).transpose() # Hay q igualarlo porq append() devuelve nuevo df
stats_df

"""Hay columnas con medias muy bajas, es decir, casi todo son ceros, por lo que no aportan gran información. Por ello se van a eliminar las columnas de *hourly*, *employer_provided* y *R_yn*. 

Además, la columna de desc_len contiene la longitud de la descripción del puesto de trabajo, que para nuestro estudio no es significativa, por lo que eliminamos esta columna. 
"""

data.drop(['hourly', 'employer_provided', 'R_yn','min_salary','avg_salary', 'desc_len'], inplace=True, axis=1)
data.head()

"""**Matriz de correlación de datos numéricos**"""

var_num = data.select_dtypes(include = ['int64','float64']).reset_index(drop = True)
corr = var_num.corr()
corr.style.background_gradient (cmap = 'coolwarm')

"""**4. Análisis de los datos.**


**4.1.** Selección de los grupos de datos que se quieren analizar/comparar (p.
ej., si se van a comparar grupos de datos, ¿cuáles son estos grupos y
qué tipo de análisis se van a aplicar?)


**4.2.** Comprobación de la normalidad y homogeneidad de la varianza.


**4.3.** Aplicación de pruebas estadísticas para comparar los grupos de datos.
En función de los datos y el objetivo del estudio, aplicar pruebas de
contraste de hipótesis, correlaciones, regresiones, etc. Aplicar al menos
tres métodos de análisis diferentes.

En este caso vamos a comparar los sueldos de los diferentes job_title. Por ello vamos a coger los 2 puestos de trabajo más ofertados y vamos a hacer contrastes de los sueldos máximos.
"""

plt.hist(data['max_salary'])
plt.show()

data.groupby(['Job Title'])['Job Title'].count().reset_index(name='count').sort_values(['count'], ascending=False).head(5)

"""Vamos a optar por el los puestos de Data Scientist y Data Engineer."""

df_DS = data[data['Job Title']=='Data Scientist']
df_DE = data[data['Job Title']=='Data Engineer']

"""Comprobamos la normalidad y homogeneidad de la varianza."""

bins = np.linspace(min(df_DS['max_salary']), max(df_DS['max_salary']), 25)
plt.hist(df_DS['max_salary'], bins, label='Data Scientist')
plt.hist(df_DE['max_salary'], bins, label='Data Engineer')
plt.legend(loc='upper right')
plt.show()

from scipy import stats
import numpy as np 
import pylab 
print(stats.shapiro(df_DS['max_salary']))
print(stats.shapiro(df_DE['max_salary']))

stats.probplot(df_DS['max_salary'],dist="norm", plot=pylab)
pylab.show()
stats.probplot(df_DE['max_salary'],dist="norm", plot=pylab)
pylab.show()

"""Dado que la prueba de Shapiro-Wilk se considera más robusta, una posición más conservadora concluiría que los datos para el puesto de Data Scientist no siguen una distribución normal. No obstante, como el conjunto de datos supera los 30 registros, por el teorema central del límite, se podría considerar que los datos siguen una distribución normal."""

print(stats.levene(df_DS['max_salary'],df_DE['max_salary']))
print(stats.fligner(df_DS['max_salary'],df_DE['max_salary']))

"""Dado que ambas pruebas resultan en un p-valor inferior al nivel de significancia (< 0,05), se rechaza la hipótesis nula de homocedasticidad y se concluye que la variable max_salary presenta varianzas estadísticamente diferentes para los diferentes grupos de Job Title.

Nuestra hipotesis es que el salario máximo de los Data Scientist es superior a los Data Engineers, es decir, esta es nuestra hipotesis alternativa. Por ello, vamos a usar el T-test con varianzas diferentes para comprobarlo.
"""

stats.ttest_ind(df_DS['max_salary'],df_DE['max_salary'],equal_var=False,alternative='greater')

"""Como el p-value es menor a 0.05, aceptamos la hipotesis alternativa y concluimos que tenemos suficientes evidencias para afirmar que el salario máximo de los Data Scientist es superior a los Data Engineers."""

#var_num = df_DS.select_dtypes(include = ['int64','float64']).reset_index(drop = True)
corr = df_DS.corr()
corr.style.background_gradient (cmap = 'coolwarm')

var_num = df_DE.select_dtypes(include = ['int64','float64']).reset_index(drop = True)
corr = var_num.corr()
corr.style.background_gradient (cmap = 'coolwarm')

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

X = df_DE.drop(['max_salary'],axis=1)
y = df_DE['max_salary']

X_encoded = pd.get_dummies(X, drop_first=True)
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

regressor = RandomForestRegressor(random_state=42) 
estimators = np.arange(10, 200, 10)
scores = []
for n in estimators:
    regressor.set_params(n_estimators=n)
    regressor.fit(X_train, y_train)
    scores.append(regressor.score(X_test, y_test))
plt.title("Effect of n_estimators Data Engineer Jobs")
plt.xlabel("n_estimator")
plt.ylabel("score")
plt.plot(estimators, scores)

X = df_DS.drop(['max_salary'],axis=1)
y = df_DS['max_salary']

X_encoded = pd.get_dummies(X, drop_first=True)
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

regressor = RandomForestRegressor(random_state=42) 
estimators = np.arange(10, 200, 10)
scores = []
for n in estimators:
    regressor.set_params(n_estimators=n)
    regressor.fit(X_train, y_train)
    scores.append(regressor.score(X_test, y_test))
plt.title("Effect of n_estimators Data Scientist Jobs")
plt.xlabel("n_estimator")
plt.ylabel("score")
plt.plot(estimators, scores)

X = data.drop(['max_salary'],axis=1)
y = data['max_salary']

X_encoded = pd.get_dummies(X, drop_first=True)
X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

regressor = RandomForestRegressor(random_state=42) 
estimators = np.arange(10, 200, 10)
scores = []
for n in estimators:
    regressor.set_params(n_estimators=n)
    regressor.fit(X_train, y_train)
    scores.append(regressor.score(X_test, y_test))
plt.title("Effect of n_estimators all jobs")
plt.xlabel("n_estimator")
plt.ylabel("score")
plt.plot(estimators, scores)

"""Podemos ver que el modelo de regressión propuesto alcanza un accuracy elevado con un valor bajo de estimadores. si aumentamos dicho parámetro obtenemos una mejora del accuracy que no es significante.

**6. Resolución del problema.** A partir de los resultados obtenidos, ¿cuáles son
las conclusiones? ¿Los resultados permiten responder al problema?

Hemos realizado varios procesos de limpieza, selección y filtrado de características con el objetivo de analizar en qué puesto se obtiene un mayor salario, si en Data Scientist o en Data Engineer. 

Con el contraste de hipótesis se obtiene que el rol de Data Scientist está mejor remunerado que el de Data Engineer. 
Además hemos creado un modelo de regresión que predice el salario máximo en función de las características del puesto de trabajo.
"""

data.to_csv('Práctica2_final.csv',index=False, sep=";")

data.head()

